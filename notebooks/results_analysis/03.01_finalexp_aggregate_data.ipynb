{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.206631Z",
     "start_time": "2026-01-30T14:42:01.201404Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "interface_dir = os.environ[\"DATA\"] + \"webinterfaces/exp02/\"\n",
    "\n",
    "tasks_dir = os.path.join(interface_dir, \"res\", \"tasks\")\n",
    "results_dir = os.path.join(interface_dir, \"results\")\n",
    "protocols_dir = os.path.join(interface_dir, \"protocols\")\n",
    "prolific_matching_dir = os.path.join(interface_dir, \"prolific\")\n",
    "\n",
    "protocol_paths_d = {\n",
    "    \"H\": os.path.join(protocols_dir, \"H_0.json\"),\n",
    "    \"H+AI\": os.path.join(protocols_dir, \"AI_0.json\"),\n",
    "    \"H+AI+CF\": os.path.join(protocols_dir, \"XAI_CF_0.json\"),\n",
    "    \"H+AI+SHAP\": os.path.join(protocols_dir, \"XAI_SHAP_0.json\"),\n",
    "    \"H+AI+LLM\": os.path.join(protocols_dir, \"XAI_LLM_0.json\"),\n",
    "    \"H+AI+GRADCAM\": os.path.join(protocols_dir, \"XAI_GRADCAM_0.json\"),\n",
    "}\n",
    "\n",
    "COMPREHENSION_THRESHOLD = 0.8\n",
    "\n",
    "COMPREHENSION_TASKS = [\"xeasy1_find_pattern_rot\"]\n",
    "TRAINING_TASKS = [\"med3_find_pattern_rot\"]\n",
    "EASY_TASKS = [\"easy1_find_pattern_rot\", \"easy3_find_pattern_rot\"]\n",
    "DIFFICULT_TASKS = [\"hard1_find_pattern_rot\", \"hard3_find_pattern_rot\"]\n",
    "\n",
    "MILD_PRESSURE_TASKS = [\"easy1_find_pattern_rot\", \"hard1_find_pattern_rot\"]\n",
    "STRONG_PRESSURE_TASKS = [\"easy3_find_pattern_rot\", \"hard3_find_pattern_rot\"]\n",
    "\n",
    "TASK_PROTOCOL_KEYS = {\n",
    "    \"easy1_find_pattern_rot\": \"mainexp_easy_mild_patrot_task\",\n",
    "    \"easy3_find_pattern_rot\": \"mainexp_easy_strong_patrot_task\",\n",
    "\n",
    "    \"hard1_find_pattern_rot\": \"mainexp_hard_mild_patrot_task\",\n",
    "    \"hard3_find_pattern_rot\": \"mainexp_hard_strong_patrot_task\",\n",
    "\n",
    "    \"xeasy1_find_pattern_rot\": \"intro_comprehension_task\",\n",
    "    \"med3_find_pattern_rot\": \"intro_training_1_task\"\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.250551Z",
     "start_time": "2026-01-30T14:42:01.248778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/jleguy/Documents/postdoc/git_repos/WebXAII/\")"
   ],
   "id": "89d45f6e88b94b0a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.297278Z",
     "start_time": "2026-01-30T14:42:01.294171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "\n",
    "def load_task_csv_file(path):\n",
    "    y_true, y_pred = [], []\n",
    "    with open(path) as csv_data:\n",
    "        reader = csv.DictReader(csv_data)\n",
    "        for row in reader:\n",
    "            y_true.append(int(row[\"target\"]))\n",
    "            y_pred.append(int(row[\"pred\"]))\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred)\n"
   ],
   "id": "2ce1441fd9d65975",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.344099Z",
     "start_time": "2026-01-30T14:42:01.340016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def data_matching(protocols_paths_d, prolific_matching_files):\n",
    "    results_filenames_d = {k: [] for k in protocol_paths_d.keys()}\n",
    "\n",
    "    for prolific_matching_file in prolific_matching_files:\n",
    "\n",
    "        with open(prolific_matching_file) as json_data:\n",
    "            d = json.load(json_data)\n",
    "\n",
    "            for prolific_id, prot_dict in d.items():\n",
    "                condition_split = prot_dict[\"protocol\"].split(\"_\")\n",
    "                filename = prolific_id + \".json\"\n",
    "\n",
    "                if condition_split[0] == \"H\":\n",
    "                    results_filenames_d[\"H\"].append(filename)\n",
    "                elif condition_split[0] == \"AI\":\n",
    "                    results_filenames_d[\"H+AI\"].append(filename)\n",
    "                elif condition_split[0] == \"XAI\" and condition_split[1] == \"SHAP\":\n",
    "                    results_filenames_d[\"H+AI+SHAP\"].append(filename)\n",
    "                elif condition_split[0] == \"XAI\" and condition_split[1] == \"CF\":\n",
    "                    results_filenames_d[\"H+AI+CF\"].append(filename)\n",
    "                elif condition_split[0] == \"XAI\" and condition_split[1] == \"LLM\":\n",
    "                    results_filenames_d[\"H+AI+LLM\"].append(filename)\n",
    "                elif condition_split[0] == \"XAI\" and condition_split[1] == \"GRADCAM\":\n",
    "                    results_filenames_d[\"H+AI+GRADCAM\"].append(filename)\n",
    "\n",
    "    return results_filenames_d\n"
   ],
   "id": "c30a20478213e391",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.392279Z",
     "start_time": "2026-01-30T14:42:01.388245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pywebxaii.resretrieval import extract_p_questionnaire_results, get_protocol_entry_from_key\n",
    "\n",
    "\n",
    "def extract_quest_results(results_dir, results_filenames_d, protocol_paths_d, quest_keys):\n",
    "    output_res_d = {}\n",
    "\n",
    "    # Iterating over all groups\n",
    "    for group_key, filenames_list in results_filenames_d.items():\n",
    "\n",
    "        output_res_d[group_key] = {\"raw\": {}, \"values\": {}, \"times\": {}}\n",
    "\n",
    "        # Iterating on all results files for the current group\n",
    "        for filename in filenames_list:\n",
    "            curr_res_path = os.path.join(results_dir, filename)\n",
    "            curr_res_d = load_json(curr_res_path)\n",
    "            data_issue = False\n",
    "            if not curr_res_d[\"is_completed\"]:\n",
    "                data_issue = True\n",
    "\n",
    "            # Iterating over all questionnaires keys\n",
    "            for quest_key in quest_keys:\n",
    "\n",
    "                curr_protocol_d = load_json(protocol_paths_d[group_key])\n",
    "                try:\n",
    "                    get_protocol_entry_from_key(curr_protocol_d, quest_key)\n",
    "                    answers_raw, answers_values, quest_times = extract_p_questionnaire_results(curr_res_d,\n",
    "                                                                                               quest_key,\n",
    "                                                                                               protocol_d=curr_protocol_d)\n",
    "                except KeyError:\n",
    "                    data_issue = True\n",
    "\n",
    "                if quest_key not in output_res_d[group_key][\"raw\"]:\n",
    "                    output_res_d[group_key][\"raw\"][quest_key] = []\n",
    "                    output_res_d[group_key][\"values\"][quest_key] = []\n",
    "                    output_res_d[group_key][\"times\"][quest_key] = []\n",
    "\n",
    "                if data_issue:\n",
    "                    output_res_d[group_key][\"raw\"][quest_key].append(None)\n",
    "                    output_res_d[group_key][\"values\"][quest_key].append(None)\n",
    "                    output_res_d[group_key][\"times\"][quest_key].append(None)\n",
    "                else:\n",
    "                    output_res_d[group_key][\"raw\"][quest_key].append(answers_raw)\n",
    "                    output_res_d[group_key][\"values\"][quest_key].append(answers_values)\n",
    "                    output_res_d[group_key][\"times\"][quest_key].append(quest_times)\n",
    "\n",
    "    return output_res_d"
   ],
   "id": "5438aa4b2c47a4f2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.444013Z",
     "start_time": "2026-01-30T14:42:01.438776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pywebxaii.resretrieval import extract_p_task_results\n",
    "\n",
    "\n",
    "def compute_scores(results_dir, results_filenames_d, protocol_paths_d, tasks_dir, tasks_names, task_protocol_keys):\n",
    "    output_res_scores_d = {}\n",
    "    output_res_reliance_d = {}\n",
    "    output_res_overreliance_d = {}\n",
    "    output_res_underreliance_d = {}\n",
    "    output_res_appropriate_reliance_d = {}\n",
    "    output_res_task_true_d = {}\n",
    "    output_res_ai_pred_d = {}\n",
    "    output_res_user_decision_d = {}\n",
    "    output_res_quest_order_d = {}\n",
    "    output_res_answer_times_d = {}\n",
    "\n",
    "    # Iterating over all groups\n",
    "    for group_key, filenames_list in results_filenames_d.items():\n",
    "\n",
    "        output_res_scores_d[group_key] = []\n",
    "        output_res_reliance_d[group_key] = []\n",
    "        output_res_overreliance_d[group_key] = []\n",
    "        output_res_underreliance_d[group_key] = []\n",
    "        output_res_appropriate_reliance_d[group_key] = []\n",
    "        output_res_answer_times_d[group_key] = []\n",
    "        output_res_task_true_d[group_key] = []\n",
    "        output_res_ai_pred_d[group_key] = []\n",
    "        output_res_user_decision_d[group_key] = []\n",
    "        output_res_quest_order_d[group_key] = []\n",
    "\n",
    "        # Iterating on all results files for the current group\n",
    "        for filename in filenames_list:\n",
    "            curr_res_path = os.path.join(results_dir, filename)\n",
    "            curr_res_d = load_json(curr_res_path)\n",
    "            if not curr_res_d[\"is_completed\"]:\n",
    "                continue\n",
    "\n",
    "            nb_questions = 0\n",
    "            nb_quest_wrong_predictions = 0\n",
    "            nb_quest_right_predictions = 0\n",
    "            nb_correct = 0\n",
    "            nb_reliance = 0\n",
    "            nb_overreliance = 0\n",
    "            nb_underreliance = 0\n",
    "            answer_times = []\n",
    "            early_break = False\n",
    "            task_true_l = []\n",
    "            ai_pred_l = []\n",
    "            user_decision_l = []\n",
    "            quest_order_l = []\n",
    "            # Iterating over all tasks\n",
    "            for task_idx, task_name in enumerate(tasks_names):\n",
    "\n",
    "                task_true, ai_pred = load_task_csv_file(os.path.join(tasks_dir, task_name + \"_content.csv\"))\n",
    "\n",
    "                answers_idx_vect, answers_text_vect, quest_order_vect, time_vect, _, _ = \\\n",
    "                    extract_p_task_results(curr_res_d,\n",
    "                                           task_protocol_keys[tasks_names[task_idx]],\n",
    "                                           protocol_d=load_json(protocol_paths_d[group_key]))\n",
    "\n",
    "                nb_questions += len(answers_idx_vect)\n",
    "                nb_quest_wrong_predictions += np.sum(task_true != ai_pred)\n",
    "                nb_quest_right_predictions += np.sum(task_true == ai_pred)\n",
    "\n",
    "                try:\n",
    "                    nb_correct += np.sum(answers_idx_vect == np.logical_not(task_true))\n",
    "                    nb_reliance += np.sum(answers_idx_vect == np.logical_not(ai_pred))\n",
    "                    nb_overreliance += np.sum(np.logical_and(\n",
    "                        answers_idx_vect == np.logical_not(ai_pred),\n",
    "                        ai_pred != task_true\n",
    "                    ))\n",
    "                    nb_underreliance += np.sum(np.logical_and(\n",
    "                        answers_idx_vect != np.logical_not(ai_pred),\n",
    "                        ai_pred == task_true\n",
    "                    ))\n",
    "                    answer_times.extend((np.array(time_vect) / 1000).tolist())\n",
    "\n",
    "                    task_true_l.append(task_true)\n",
    "                    ai_pred_l.append(ai_pred)\n",
    "                    user_decision_l.append(np.logical_not(answers_idx_vect))\n",
    "                    quest_order_l.append(quest_order_vect)\n",
    "\n",
    "                    #\n",
    "                    # if np.isnan(np.sum(answers_idx_vect)):\n",
    "                    #     print(f\"answers {answers_idx_vect}\")\n",
    "                    #     print(f\"true {np.logical_not(task_true)}\")\n",
    "                    #     print(f\"ai pred {np.logical_not(ai_pred)}\")\n",
    "                    #     print(f\"correct extracted {np.sum(answers_idx_vect == np.logical_not(task_true))}\")\n",
    "                    #     print(f\"reliance extracted {np.sum(answers_idx_vect == np.logical_not(ai_pred))}\")\n",
    "\n",
    "                # Happens if the results file is not complete\n",
    "                except ValueError:\n",
    "                    print(\"ValueError exception\")\n",
    "                    output_res_scores_d[group_key].append(None)\n",
    "                    output_res_reliance_d[group_key].append(None)\n",
    "                    output_res_overreliance_d[group_key].append(None)\n",
    "                    output_res_underreliance_d[group_key].append(None)\n",
    "                    output_res_answer_times_d[group_key].append(None)\n",
    "                    output_res_task_true_d[group_key].append(None)\n",
    "                    output_res_ai_pred_d[group_key].append(None)\n",
    "                    output_res_user_decision_d[group_key].append(None)\n",
    "                    output_res_quest_order_d[group_key].append(None)\n",
    "                    early_break = True\n",
    "                    break\n",
    "\n",
    "            if not early_break:\n",
    "                output_res_scores_d[group_key].append(nb_correct / nb_questions)\n",
    "                output_res_reliance_d[group_key].append(nb_reliance / nb_questions)\n",
    "                output_res_overreliance_d[group_key].append(nb_overreliance / nb_quest_wrong_predictions)\n",
    "                output_res_underreliance_d[group_key].append(nb_underreliance / nb_quest_right_predictions)\n",
    "                output_res_answer_times_d[group_key].extend(answer_times)\n",
    "                output_res_task_true_d[group_key].append(task_true_l)\n",
    "                output_res_ai_pred_d[group_key].extend(ai_pred_l)\n",
    "                output_res_user_decision_d[group_key].extend(user_decision_l)\n",
    "                output_res_quest_order_d[group_key].extend(quest_order_l)\n",
    "\n",
    "    return (output_res_scores_d, output_res_reliance_d, output_res_overreliance_d, output_res_underreliance_d, output_res_answer_times_d,\n",
    "            output_res_task_true_d, output_res_ai_pred_d, output_res_user_decision_d, output_res_quest_order_d)\n"
   ],
   "id": "98983859affedfd0",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.489117Z",
     "start_time": "2026-01-30T14:42:01.486036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_filenames_d = data_matching(protocol_paths_d, [os.path.join(prolific_matching_dir, \"prolific.json\"),\n",
    "                                                       os.path.join(prolific_matching_dir, \"prolific_21-1.json\"),\n",
    "                                                       os.path.join(prolific_matching_dir, \"prolific_21-2.json\"),\n",
    "                                                       os.path.join(prolific_matching_dir, \"prolific_22-1.json\"),\n",
    "                                                       os.path.join(prolific_matching_dir, \"prolific_27-1.json\"),\n",
    "                                                       os.path.join(prolific_matching_dir, \"prolific_28-1.json\"),\n",
    "                                                       ])\n",
    "\n",
    "# results_filenames_d = data_matching(protocol_paths_d, [os.path.join(prolific_matching_dir, \"prolific_28-1.json\")])"
   ],
   "id": "13d96f62949b2592",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.533684Z",
     "start_time": "2026-01-30T14:42:01.530394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "for k, v in results_filenames_d.items():\n",
    "    total += len(v)\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "print(f\"Total: {total}\")"
   ],
   "id": "387636b229bbdf5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 121\n",
      "H+AI: 108\n",
      "H+AI+CF: 115\n",
      "H+AI+SHAP: 113\n",
      "H+AI+LLM: 111\n",
      "H+AI+GRADCAM: 110\n",
      "Total: 678\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.580969Z",
     "start_time": "2026-01-30T14:42:01.577918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_not_completed(results_filenames_d):\n",
    "    filtered_results_filenames_d = {}\n",
    "\n",
    "    # Iterating over all groups\n",
    "    for group_key, filenames_list in results_filenames_d.items():\n",
    "\n",
    "        filtered_results_filenames_d[group_key] = []\n",
    "\n",
    "        # Iterating on all results files for the current group\n",
    "        for filename in filenames_list:\n",
    "            curr_res_path = os.path.join(results_dir, filename)\n",
    "            curr_res_d = load_json(curr_res_path)\n",
    "            if not curr_res_d[\"is_completed\"]:\n",
    "                continue\n",
    "            filtered_results_filenames_d[group_key].append(filename)\n",
    "    return filtered_results_filenames_d"
   ],
   "id": "f4aced40c9e94795",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.678722Z",
     "start_time": "2026-01-30T14:42:01.624150Z"
    }
   },
   "cell_type": "code",
   "source": "results_filenames_d = filter_not_completed(results_filenames_d)",
   "id": "1cc00e66d98bf416",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.685852Z",
     "start_time": "2026-01-30T14:42:01.683714Z"
    }
   },
   "cell_type": "code",
   "source": "results_filenames_before_filtering = dict(results_filenames_d)",
   "id": "94149848c730af63",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.730505Z",
     "start_time": "2026-01-30T14:42:01.727832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "for k, v in results_filenames_d.items():\n",
    "    total += len(v)\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "print(f\"Total: {total}\")"
   ],
   "id": "a4b09423e1d62f2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 99\n",
      "H+AI: 99\n",
      "H+AI+CF: 100\n",
      "H+AI+SHAP: 97\n",
      "H+AI+LLM: 99\n",
      "H+AI+GRADCAM: 95\n",
      "Total: 589\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:01.781747Z",
     "start_time": "2026-01-30T14:42:01.778089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_on_attention_tests(results_filenames_d):\n",
    "    res = extract_quest_results(results_dir, results_filenames_d, protocol_paths_d,\n",
    "                                [\"attentioncheck_1\", \"attentioncheck_2\"])\n",
    "    filtered_results_filenames_d = {}\n",
    "\n",
    "    for k, v in res.items():\n",
    "        filtered_results_filenames_d[k] = []\n",
    "        for i in range(len(results_filenames_d[k])):\n",
    "            if v[\"raw\"][\"attentioncheck_1\"][i] is None:\n",
    "                passes1 = False\n",
    "            else:\n",
    "                passes1 = v[\"raw\"][\"attentioncheck_1\"][i][0] == 2 and v[\"raw\"][\"attentioncheck_1\"][i][1] == 0\n",
    "\n",
    "            if v[\"raw\"][\"attentioncheck_2\"][i] is None:\n",
    "                passes2 = False\n",
    "            else:\n",
    "                passes2 = v[\"raw\"][\"attentioncheck_2\"][i][0] == 6 and v[\"raw\"][\"attentioncheck_2\"][i][1] == 0\n",
    "\n",
    "            if passes1 and passes2:\n",
    "                filtered_results_filenames_d[k].append(results_filenames_d[k][i])\n",
    "\n",
    "    return filtered_results_filenames_d"
   ],
   "id": "d745393ca24c2d92",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.096341Z",
     "start_time": "2026-01-30T14:42:01.823798Z"
    }
   },
   "cell_type": "code",
   "source": "results_filenames_d = filter_on_attention_tests(results_filenames_d)",
   "id": "a882d5c1004aea09",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.103490Z",
     "start_time": "2026-01-30T14:42:02.101279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "for k, v in results_filenames_d.items():\n",
    "    total += len(v)\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "print(f\"Total: {total}\")"
   ],
   "id": "79ea6b3c47892d95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 88\n",
      "H+AI: 94\n",
      "H+AI+CF: 89\n",
      "H+AI+SHAP: 91\n",
      "H+AI+LLM: 95\n",
      "H+AI+GRADCAM: 90\n",
      "Total: 547\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.149972Z",
     "start_time": "2026-01-30T14:42:02.146799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Success rate at attention tests :\")\n",
    "for k, v in results_filenames_d.items():\n",
    "    total += len(v)\n",
    "    print(f\"{k}: {len(v)/len(results_filenames_before_filtering[k])*100:.2f}%\")"
   ],
   "id": "a2ca65931ceb46ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate at attention tests :\n",
      "H: 88.89%\n",
      "H+AI: 94.95%\n",
      "H+AI+CF: 89.00%\n",
      "H+AI+SHAP: 93.81%\n",
      "H+AI+LLM: 95.96%\n",
      "H+AI+GRADCAM: 94.74%\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.197774Z",
     "start_time": "2026-01-30T14:42:02.194363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_comprehension_score(results_filenames_d):\n",
    "    comprehension_score_d, _, _, _, _, _, _, _, _= compute_scores(results_dir, results_filenames_d, protocol_paths_d, tasks_dir,\n",
    "                                                       COMPREHENSION_TASKS, TASK_PROTOCOL_KEYS)\n",
    "    filtered_results_filenames_d = {}\n",
    "\n",
    "    for k, v in comprehension_score_d.items():\n",
    "        filtered_results_filenames_d[k] = []\n",
    "        for i in range(len(results_filenames_d[k])):\n",
    "            if comprehension_score_d[k][i] >= COMPREHENSION_THRESHOLD:\n",
    "                filtered_results_filenames_d[k].append(results_filenames_d[k][i])\n",
    "            else:\n",
    "                print(f\"Rejecting sample due to comprehension score of {comprehension_score_d[k][i]}\")\n",
    "\n",
    "    return filtered_results_filenames_d\n"
   ],
   "id": "8e42d3b3ae0595c4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.427432Z",
     "start_time": "2026-01-30T14:42:02.242345Z"
    }
   },
   "cell_type": "code",
   "source": "results_filenames_d = filter_comprehension_score(results_filenames_d)",
   "id": "83c990e8fa682793",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.4\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n",
      "Rejecting sample due to comprehension score of 0.6\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.437477Z",
     "start_time": "2026-01-30T14:42:02.435440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "for k, v in results_filenames_d.items():\n",
    "    total += len(v)\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "print(f\"Total: {total}\")"
   ],
   "id": "fdf91c3b500c23b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 85\n",
      "H+AI: 89\n",
      "H+AI+CF: 80\n",
      "H+AI+SHAP: 86\n",
      "H+AI+LLM: 85\n",
      "H+AI+GRADCAM: 85\n",
      "Total: 510\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:02.544078Z",
     "start_time": "2026-01-30T14:42:02.540951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Total passing filters among complete files: {np.sum([len(v) for v in results_filenames_d.values()])}/{np.sum([len(v) for v in results_filenames_before_filtering.values()])}\")"
   ],
   "id": "b0631b6c24dbfcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total passing filters among complete files: 510/589\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:03.216394Z",
     "start_time": "2026-01-30T14:42:02.645514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_scores, all_reliance, all_overreliance, all_underreliance, all_times, \\\n",
    "    all_task_true, all_ai_pred, all_user_decision, all_quest_order = compute_scores(results_dir,\n",
    "                                                                                          results_filenames_d,\n",
    "                                                                                          protocol_paths_d,\n",
    "                                                                                          tasks_dir,\n",
    "                                                                                          EASY_TASKS + DIFFICULT_TASKS,\n",
    "                                                                                          TASK_PROTOCOL_KEYS)"
   ],
   "id": "93cc82e41de79ac8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:32.932843Z",
     "start_time": "2026-01-30T14:42:31.812388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores_easy_mild, reliance_easy_mild, overreliance_easy_mild, underreliance_easy_mild, answertimes_easy_mild, \\\n",
    "    task_true_easy_mild, ai_pred_easy_mild, user_decision_easy_mild, quest_order_easy_mild = compute_scores(\n",
    "    results_dir, results_filenames_d, protocol_paths_d, tasks_dir, [\"easy1_find_pattern_rot\"], TASK_PROTOCOL_KEYS)\n",
    "scores_easy_strong, reliance_easy_strong, overreliance_easy_strong, underreliance_easy_strong, answertimes_easy_strong, \\\n",
    "    task_true_easy_strong, ai_pred_easy_strong, user_decision_easy_strong, quest_order_easy_strong = compute_scores(\n",
    "    results_dir, results_filenames_d, protocol_paths_d, tasks_dir, [\"easy3_find_pattern_rot\"], TASK_PROTOCOL_KEYS)\n",
    "scores_hard_mild, reliance_hard_mild, overreliance_hard_mild, underreliance_hard_mild, answertimes_hard_mild, \\\n",
    "    task_true_hard_mild, ai_pred_hard_mild, user_decision_hard_mild, quest_order_hard_mild = compute_scores(\n",
    "    results_dir, results_filenames_d, protocol_paths_d, tasks_dir, [\"hard1_find_pattern_rot\"], TASK_PROTOCOL_KEYS)\n",
    "scores_hard_strong, reliance_hard_strong, overreliance_hard_strong, underreliance_hard_strong, answertimes_hard_strong, \\\n",
    "    task_true_hard_strong, ai_pred_hard_strong, user_decision_hard_strong, quest_order_hard_strong = compute_scores(\n",
    "    results_dir, results_filenames_d, protocol_paths_d, tasks_dir, [\"hard3_find_pattern_rot\"], TASK_PROTOCOL_KEYS)\n",
    "\n",
    "comprehension_score, _, _, _, _, _, _, _, _ = compute_scores(results_dir, results_filenames_d, protocol_paths_d, tasks_dir,\n",
    "                                                 COMPREHENSION_TASKS, TASK_PROTOCOL_KEYS)\n"
   ],
   "id": "966e8d708e762139",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:42:04.110556783Z",
     "start_time": "2026-01-29T16:04:42.001586Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3d0c0ab427c45635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6da3ce661792dc54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
